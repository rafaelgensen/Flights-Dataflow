# ğŸš€ Data Engineering Pipelines with Apache Beam, Dataflow and Pub/Sub

![Project Architecture](assets/pipelines.png)

Repository containing hands-on exercises and projects from the **Apache Beam + Dataflow + Pub/Sub (Udemy)** course.  
The focus is to understand **batch and streaming data processing** on Google Cloud, building end-to-end pipelines.

---

## ğŸ“‚ Repository Structure
- `batch/` â†’ Beam batch pipelines
  - Local â†’ Local
  - Local â†’ Google Cloud Storage (GCS)
  - GCS â†’ GCS
  - GCS â†’ BigQuery
- `streaming/` â†’ streaming pipelines
  - Producer â†’ publishes messages to Pub/Sub
  - Consumer â†’ reads messages from Pub/Sub
  - Pipeline â†’ integrates **Pub/Sub + Dataflow + Pub/Sub**

---

## ğŸ› ï¸ Technologies Used
- **Apache Beam** â†’ unified programming SDK for batch + streaming pipelines  
- **Google Cloud Dataflow** â†’ fully managed execution of Beam pipelines  
- **Google Cloud Pub/Sub** â†’ messaging system for event ingestion and consumption  
- **Google Cloud Storage (GCS)** â†’ data lake for raw and processed data  
- **BigQuery** â†’ data warehouse for analytics and querying  

---

## ğŸ“˜ Key Concepts Practiced
### ğŸ”¹ Batch
- Building simple pipelines with Beam  
- Data movement between:
  - Local files
  - GCS buckets
  - BigQuery  
- Deploying pipelines on Dataflow for scalable execution  

### ğŸ”¹ Streaming
- Creating a **Publisher** in Pub/Sub to simulate real-time events  
- Developing a **Consumer** to process Pub/Sub messages  
- End-to-end integration **Pub/Sub â†’ Dataflow â†’ Pub/Sub** for continuous data flow  

---

## ğŸ’¡ Example Use Cases
- **Batch:** Daily ETL job moving CSV files from GCS â†’ transform with Beam â†’ load into BigQuery.  
- **Streaming:** Real-time IoT/Log simulation with Pub/Sub, consumed by Dataflow for continuous processing.  